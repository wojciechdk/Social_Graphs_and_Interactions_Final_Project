{"cells":[{"source":["# Downloading WikiPedia Pages and extracting links\n","\n","Now that we generated the list of pages we want, we need to download them, parse them for links, and save them in a reasonable format."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from typing import Dict\n","import regex as re\n","from pathlib import Path\n","import os\n","from tqdm.auto import tqdm\n","import json\n","from library_functions.config import Config"]},{"source":["We are still using the [MediaWiki](https://github.com/barrust/mediawiki) library, unfortunately it had some problems that prevented us from doing what we wanted:\n","\n","1. The intro section of a wikipedia page was not parsed by the library. This meant that for many short pages, who *only* had an introduction, the library did not return anything.\n","\n","2. When parsing links in separate sections of a page, the library re-downloaded the *whole* page for every section. This made it prohibitively slow.\n","\n","\n","But fear not: through the power of Open-Source software, we cloned the library and fixed it! This resulted in two pull requests, [1](https://github.com/barrust/mediawiki/pull/90) and [2](https://github.com/barrust/mediawiki/pull/91). We are proud to say that we contributed to FOSS as part of our assignment.\n","\n","One of the pull requests was approved, the other one is still pending, so in order to use the library, we need to import it from our local, modified version:\n"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sys\n","sys.path.append(\"/home/ldorigo/MEGA/DTU/Q2/social_graphs/mediawiki/\")\n","from mediawiki import MediaWiki"]},{"source":["Load in the category tree we generated previously:"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open(Config.Path.full_category_tree_clean, \"r\") as f:\n","    root_tree = json.load(f)"]},{"source":["## Utility Functions to traverse the category tree and download pages\n","\n","### Getting inline links\n","The 'official' MediaWiki API, when asked for links from a page, returns outgoing links that include those in template boxes at the end of pages (See at the bottom of [this page](https://en.wikipedia.org/wiki/Caffeine#External_links) to understand what we mean).\n","\n","This adds an enormous amount of links that aren't relevant for out network, and result in a big mess. We thus need to extract the links \"by hand\", by using the python library's function that we adapted in one of our pull requests."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_inline_links(page: MediaWiki.page):\n","    # Store the links given by the \"official\" API\n","    api_links = page.links\n","\n","    # Only get links from sections that are actually on-topic\n","    relevant_sections = [\n","        section\n","        for section in page.sections\n","        if section not in [\n","            \"See also\", \n","            \"References\", \n","            \"Bibliography\", \n","            \"External links\"\n","            ]\n","    ]\n","\n","    # Also parse the intro section (0) \n","    relevant_sections.append(0)\n","\n","    actual_links = {}\n","    for section in tqdm(relevant_sections):\n","        # Start by getting all links\n","        tentative_links = page.parse_section_links(section)\n","\n","        if not tentative_links:\n","            continue\n","\n","        # Remove all references and external links\n","        no_references = []\n","        for name, url in tentative_links:\n","            if not re.search(\"\\[[0-9]*\\]\", name) and not re.search(\"^FILE\", name):\n","                if re.search(\"en\\.wikipedia\\.org\", url):\n","                    no_references.append((name, url))\n","\n","        for name, url in no_references:\n","            # Get the wikipedia title from the http link            \n","            link_title = re.search(\"/([^/]+)(/?)$\", url)\n","            if link_title:\n","                # Wikipedia titles have spaces, not underscores\n","                actual_title = link_title[1].replace(\"_\", \" \")\n","                # Sanity check: make sure the link we extracted \n","                # is part of the links returned by the API\n","                if actual_title not in api_links:\n","                    continue\n","                # We also keep track of how many times a specific link is used,\n","                # in case we want to use that information later on\n","                actual_links[actual_title] = (\n","                    actual_links.setdefault(actual_title, 0) + 1\n","                )\n","    return actual_links"]},{"source":["Now let's write a couple of functions to traverse our tree and download pages along the way.\n","\n","Firsy, to download and save a single page:"],"cell_type":"markdown","metadata":{}},{"source":["\n","def process_page(name: str, current_dir: Path, redirects: Dict) -> Dict:\n","    # Path where we want to write the file\n","    filepath = current_dir.joinpath(name.replace(\"/\", \"_\") + \".json\")\n","\n","    # To enable resuming: if the article was already downloaded, \n","    # just read it from file\n","    if filepath.exists():\n","        with open(filepath.as_posix(), \"r\") as f:\n","            return json.load(f)\n","\n","    # Get the page through the API\n","    page = mw.page(name)\n","    results = {}\n","\n","    ## Add redirects to the global list of synonyms\n","    for r in page.redirects:\n","        redirects[r] = name\n","\n","    ## Save all outgoing links\n","    results[\"links\"] = get_inline_links(page)\n","\n","    ## Save \"secondary\" categories of the page\n","    results[\"categories\"] = page.categories\n","\n","    ## Also save a reference to the redirects within the page itself\n","    results[\"redirects\"] = page.redirects\n","\n","    ## Finally, save the contents and the url for quick reference:\n","    results[\"url\"] = page.url\n","    results[\"content\"] = page.content\n","\n","    ## Save to a json file\n","    with open(filepath, \"w+\") as f:\n","        json.dump(results, f)\n","    return results"],"cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":["And now a function that recursively traverses the category tree:"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def process_tree(\n","    root_name: str, root_node: Dict, current_dir: Path, redirects: Dict\n",") -> Dict:\n","\n","    results = {}\n","\n","    # Go through all links (pages) in the current level and process them:\n","    for link in tqdm(root_node[\"links\"]):\n","        results[link] = {}\n","        results[link][\"category\"] = root_name\n","        results[link][\"data\"] = process_page(link, current_dir, redirects)\n","\n","    # Then, go through all sub-categories, and process them \n","    # using this same function recursively\n","    for sub_category in tqdm(root_node[\"sub-categories\"]):\n","        # Create a directory for each category\n","        new_dir = current_dir.joinpath(sub_category)\n","        if not new_dir.exists():\n","            os.mkdir(new_dir)\n","        # recursive call   \n","        cat_results = process_tree(\n","            root_name=sub_category,\n","            root_node=root_node[\"sub-categories\"][sub_category],\n","            current_dir=new_dir,\n","            redirects=redirects,\n","        )\n","        results.update(cat_results)\n","    return results"]},{"source":["Last but not least, run our function, saving all data to file as well as in a separate data structure. Note that we are saving two things:\n","\n","1. The raw data, with one page per file under the wiki_data directory\n","2. The full dataset in a more readily accessible form, which we will store as a large json file"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Keep a flat dict of redirects (synonyms) for later reference\n","redirects = {}\n","\n","data_path = Config.Path.private_data_folder / \"full_wiki_data\"\n","if not data_path.exists():\n","    os.mkdir(data_path)\n","\n","mw = MediaWiki()\n","content_tree = process_tree(\n","    root_name=\"custom_root\",\n","    root_node=root_tree,\n","    current_dir=data_path,\n","    redirects=redirects,\n",")\n"]},{"source":["After hand, we realized that we had overlooked some pages that also needed to be removed - so we remove them here rather than re-doing everything:"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Clean some pages that were overlooked\n","\n","del content_tree[\"Antidepressant\"]\n","del content_tree[\"Reversible inhibitor of monoamine oxidase A\"]\n","del content_tree[\"Stimulant\"]\n","del content_tree[\"Anxiolytic\"]\n","del content_tree[\"Histone deacetylase inhibitor\"]\n","del content_tree[\"Norepinephrine–dopamine disinhibitor\"]\n","del content_tree[\"Norepinephrine–dopamine reuptake inhibitor\"]\n","del content_tree[\"Monoamine oxidase inhibitor\"]\n","del content_tree[\"Heterocyclic antidepressant\"]\n","del content_tree[\"Tricyclic antidepressant\"]\n","del content_tree[\"Barbiturate\"]\n","del content_tree[\"Aphrodisiac\"]\n","del content_tree[\"Tricyclic antidepressant\"]"]},{"source":["Save the full data to a single file:"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open(Config.Path.full_wiki_data, \"w+\") as f:\n","    json.dump(content_tree, f, indent=2)\n"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6-final"},"orig_nbformat":2,"kernelspec":{"name":"python3","display_name":"Python 3"}}}