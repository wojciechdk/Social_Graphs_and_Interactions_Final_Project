{"cells":[{"source":["# Downloading 100.000+ reddit posts\n","\n","Now that we have the WikiPedia data, we need to obtain our Reddit Posts.\n","\n","We do so using the [Pushift Python Library](https://github.com/pushshift/api), which is an interface to the [Pushift.io](https://api.pushshift.io/) API which has a huge dataset of all public Reddit posts and comments."],"cell_type":"markdown","metadata":{}},{"source":["import json\n","import os\n","from typing import Generator\n","from psaw import PushshiftAPI\n","\n","try:\n","    from library_functions.config import Config\n","except ModuleNotFoundError:\n","    from project.library_functions.config import Config\n"],"cell_type":"code","metadata":{},"execution_count":2,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'psaw'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b6407f5ac002>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpsaw\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPushshiftAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'psaw'"]}]},{"source":["Initialize the API interface:"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["api = PushshiftAPI()"]},{"source":["Create a folder for our data. Due to the volume of data, this isn't on our repository."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["reddit_path = Config.Path.private_data_folder / \"reddit_data\"\n","os.mkdir(reddit_path)"]},{"source":["Note that we also made functions to download and analyze *comments*, not only posts. However, that increases our dataset by a factor of 10 (1.000.000+ comments), and therefore we found it unpractical to operate on and analyze in such a short timeframe."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["reddit_comments_path = reddit_path.joinpath(\"comments\")\n","reddit_submissions_path = reddit_path.joinpath(\"submissions\")\n","\n","os.mkdir(reddit_comments_path)\n","os.mkdir(reddit_submissions_path)"]},{"source":["Let's make two utility functions to save the posts/comments returned by the API in a more convenient form:"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def save_submissions(submission):\n","    relevant_dict = {\n","        \"title\": submission.title,\n","        \"author\": submission.author,\n","        \"timestamp\": submission.created,\n","        \"id\": submission.id,\n","    }\n","    try:\n","        relevant_dict[\"content\"] = submission.selftext\n","    except:\n","        relevant_dict[\"content\"] = \"\"\n","    with open(\n","        reddit_submissions_path.joinpath(relevant_dict[\"id\"] + \".json\"), \"w+\"\n","    ) as f:\n","        json.dump(relevant_dict, f)\n","\n","\n","def save_comment(comment):\n","    relevant_dict = {\n","        \"author\": comment.author,\n","        \"body\": comment.body,\n","        \"timestamp\": comment.created,\n","        \"id\": \"c__\" + comment.id,\n","    }\n","    with open(\n","        reddit_comments_path.joinpath(relevant_dict[\"id\"] + \".json\"), \n","        \"w+\"\n","        ) as f:\n","        json.dump(relevant_dict, f)\n","\n"]},{"source":["Getting all the submissions (or comments) is as easy as specifying the subreddit we are interested in, and passing a list of the fields we want to retrieve. This returns a generator which downloads the posts as it is iterated over:"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_submissions = api.search_submissions(\n","    subreddit=\"nootropics\",\n","    filter=[\"title\", \"author\", \"created_utc\", \"id\", \"selftext\", \"url\"],\n",")\n","\n","all_comments = api.search_comments(\n","    subreddit=\"nootropics\",\n","    filter=[\"author\", \"created_utc\", \"id\", \"body\", \"url\"],\n",")"]},{"source":["And now, go through all the submissions above and save them to file:"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def download_all_submissions(sub_generator: Generator):\n","    for i, sub in enumerate(sub_generator):\n","\n","        if i % 100 == 0:\n","            print(f\"Downloading submissions {i} - {i+100}.\")\n","\n","        save_submissions(sub)\n","\n","\n","def download_all_comments(com_generator: Generator):\n","    for i, comment in enumerate(com_generator):\n","\n","        if i % 100 == 0:\n","            print(f\"Downloading comments {i} - {i+100}.\")\n","\n","        save_comment(comment)\n","\n","download_all_comments(all_comments)\n","download_all_submissions(all_submissions)"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6-final"},"orig_nbformat":2,"kernelspec":{"name":"python3","display_name":"Python 3"}}}