{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#\n","import spacy\n","import json\n","from json import JSONDecodeError\n","\n","try:\n","    from library_functions.config import Config\n","except ModuleNotFoundError:\n","    from project.library_functions.config import Config\n","from tqdm.auto import tqdm\n","from spacy.matcher import PhraseMatcher\n","from pathlib import Path\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Initialize a spacy matcher. We want to match on lowercase tokens.\n","matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\", validate=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the list of synonyms  and titles from file\n","with open(Config.Path.synonym_mapping) as f:\n","    synonyms = json.load(f)\n","\n","with open(Config.Path.substance_names) as f:\n","    names = json.load(f)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make a dictionary that maps substances to a list of spacy docs containing their synonym\n","words_to_patterns = {synonyms[i]: [] for i in synonyms}\n","words_to_patterns.update({name: [] for name in names})\n","\n","for synonym in tqdm(synonyms):\n","    word = synonyms[synonym]\n","    words_to_patterns[word].append(nlp.make_doc(synonym))\n","\n","for name in tqdm(names):\n","    words_to_patterns[name].append(nlp.make_doc(name))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Add all patterns to the matcher\n","\n","for word in tqdm(words_to_patterns):\n","    matcher.add(word, words_to_patterns[word])\n","\n","\n","submissions_path = Path().cwd() / \"private_data\" / \"reddit_data\" / \"submissions\"\n","comments_path = Config.Path.private_data_folder / \"reddit_data\" / \"comments\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","submission_files = list(submissions_path.glob(\"**/*\"))\n","# comments_files = list(comments_path.glob(\"**/*\"))\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_submissions_generator(submission_files):\n","    for file in tqdm(submission_files):\n","        with open(file, \"r\") as f:\n","            try:\n","                yield (json.load(f), file)\n","            except JSONDecodeError:\n","                pass\n","    try:\n","        for file in tqdm(comments_files):\n","            with open(file, \"r\") as f:\n","                try:\n","                    yield (json.load(f), file)\n","                except JSONDecodeError:\n","                    pass\n","    except NameError:\n","        pass\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_submission_doc_generator(submissions_generator):\n","    for submission, path in submissions_generator:\n","        try:\n","            text = submission[\"title\"] + \" \" + submission[\"content\"]\n","        except:\n","            text = submission[\"body\"]\n","        yield (nlp.make_doc(text), submission, path)\n","        # yield nlp(test_text)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["submission_generator = get_submissions_generator(submission_files=submission_files)\n","submission_doc_generator = get_submission_doc_generator(submission_generator)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["match_generator = (\n","    (matcher(text), submission, path, text)\n","    for text, submission, path in submission_doc_generator\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","submissions_dict = {}\n","for matches, submission, path, doc in match_generator:\n","    # Get the found mathches actual name\n","    matches_resolved = [matcher.vocab[match[0]].text for match in matches]\n","    # Eliminate duplicates\n","    matches_unique = list(set(matches_resolved))\n","    if not matches_unique:\n","        continue\n","    # Add to the submission and save back to file. Also add to large reddit dictionnary\n","    submission[\"matches\"] = matches_unique\n","    submission[\"n_of_words\"] = len(doc)\n","    with open(path, \"w\") as f:\n","        json.dump(submission, f)\n","    submissions_dict[submission[\"id\"]] = submission"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save full reddit data to file:\n","\n","with open(Config.Path.reddit_data_with_NER, \"w+\") as f:\n","    json.dump(submissions_dict, f)\n","# with open(Config.Path.reddit_data_with_NER, \"r+\") as f:\n","# submissions_dict = json.load( f)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Let's save a  mapping between substances and posts in which they appear\n","\n","posts_per_substance = {substance: [] for substance in names}\n","\n","for id in submissions_dict:\n","    for substance in submissions_dict[id][\"matches\"]:\n","        posts_per_substance[substance].append(id)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open(Config.Path.posts_per_substance, \"w+\") as f:\n","    json.dump(posts_per_substance, f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}