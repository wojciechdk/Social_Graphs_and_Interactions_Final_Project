{"cells":[{"source":["# Processing Reddit Posts to find mentions of Nootropics\n","\n","The posts we downloaded from reddit are plain-text content, and there is nothing like on WikiPedia that allows us to detect a \"link\" - nootropics are mentionned in free-text, with many different names referring to the same substance, and with inconsistent casing and formatting. \n","\n","To be able to build a graph from the posts, the first step is thus to detect mentions of nootropics in our posts, for which we use [Spacy NLP](https://spacy.io/), a very nice library that can handle a large amount of NLP-related tasks at very high speed (for comparison, the tokenization of our data took around 20 minutes with Spacy, and would have taken around 6 hours with NLP). "],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import spacy\n","import json\n","from json import JSONDecodeError\n","\n","try:\n","    from library_functions.config import Config\n","except ModuleNotFoundError:\n","    from project.library_functions.config import Config\n","from tqdm.auto import tqdm\n","from spacy.matcher import PhraseMatcher\n","from pathlib import Path"]},{"source":["Spacy relies on a *language model* object, which holds all the static language-related data such as tokenization rules, statistical models, etc.\n","\n","If you want to replicate this at home, you will need to install the model as described [here](https://spacy.io/usage)."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the vocabulary\n","nlp = spacy.load(\"en_core_web_sm\")"]},{"source":["## Named Entity Recognition with PhraseMatchers\n","\n","What we want to do is called, in NLP parlance, *Named Entity Recognition (NER)*. As the name implies, the task is to detect *named enities*  (in this case, nootropics) in text. Nowadays, most NER-engines are based on statistical, machine-learning based models which provide much greater sensitivity - given enough training data, they are able to recognize entities of a specific type without having been given a specific list of such entities, and they are also much better at handling alternative spellings, synonyms, badly formatted text as well as typos.\n","\n","Due to the limited scope of this project, we don't have the time or resources to train a model that recognizes nootropics. Instead, we use another powerful feature of Spacy: [Rule-Based Matchers](https://spacy.io/usage/rule-based-matching), more specifically [Phrase Matchers](https://spacy.io/usage/rule-based-matching#phrasematcher). \n","\n","A PhraseMatcher can be given a list of entities, and for each entity, a list of phrases that will be recognized as an instance of that entity. The fact that they can recognize phrases rather than single words is essential in our case, as many nootropics have compound names (or, for instance, *caffeinated drink*  needs to be resolved to *caffeine*). It is also possible to choose which word property the matcher should use: *tokens* (for the literal text), *lowercase tokens*, *lemmas*, and more. In hindsight, it would have made most sense to use lemmas - as they are much less sensitive to typos and to inflections. We actually noticed very late that we were instead using the `LOWER` attribute, and by that time, changing such an early step would mean that we would have to re-write many aspects of our analysis, so we left it as-is.\n","\n","Once it's initialized, a sequence of texts can be piped into the matcher, which will return for each text the entities that were recognized.\n","\n","Let's initialize the phrasematcher:"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize a spacy matcher. We want to match on lowercase tokens.\n","matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\", validate=True)"]},{"source":["WikiPedia pages store a list of *redirects*, which are used internally by WikiPedia to redirect people to an article when they search for one of the redirects. This is very convenient, as it effectively provides us with a list of synonyms that we can use. Let's load that list (which we saved in a previous step) as well as the list of all substance names:"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the list of synonyms  and titles from file\n","with open(Config.Path.synonym_mapping) as f:\n","    synonyms = json.load(f)\n","\n","with open(Config.Path.substance_names) as f:\n","    names = json.load(f)"]},{"source":["Before a phrase can be added to the PhraseMatcher, it needs to be parsed (processed) by Spacy:"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make a dictionary that maps substances to a list of spacy docs containing their synonym\n","words_to_patterns = {synonyms[i]: [] for i in synonyms}\n","words_to_patterns.update({name: [] for name in names})\n","\n","for synonym in tqdm(synonyms):\n","    word = synonyms[synonym]\n","    words_to_patterns[word].append(nlp.make_doc(synonym))\n","\n","for name in tqdm(names):\n","    words_to_patterns[name].append(nlp.make_doc(name))"]},{"source":["Finally, add all patterns to the matcher:"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for word in tqdm(words_to_patterns):\n","    matcher.add(word, words_to_patterns[word])"]},{"source":["Get a list of all the files we need to process. Note that as before, we also wrote code for processing the comments - but we do not use it."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["submissions_path = Path().cwd() / \"private_data\" / \"reddit_data\" / \"submissions\"\n","# comments_path = Config.Path.private_data_folder / \"reddit_data\" / \"comments\"\n","\n","submission_files = list(submissions_path.glob(\"**/*\"))\n","# comments_files = list(comments_path.glob(\"**/*\"))"]},{"source":["To efficiently process posts, they need to be fed to the phrasematcher by a *generator* - which is like a list, but whose elements are generated on the fly, here by reading them from disk."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function that returns a generators that yields raw reddit oposts\n","def get_submissions_generator(submission_files):\n","    for file in tqdm(submission_files):\n","        with open(file, \"r\") as f:\n","            try:\n","                yield (json.load(f), file)\n","            except JSONDecodeError:\n","                pass\n","    # If comments are used as well, continue to feed those\n","    try:\n","        for file in tqdm(comments_files):\n","            with open(file, \"r\") as f:\n","                try:\n","                    yield (json.load(f), file)\n","                except JSONDecodeError:\n","                    pass\n","    except NameError:\n","        pass"]},{"source":["The results from that generator are passed into a new generator, which uses spacy to tokenize them:"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_submission_doc_generator(submissions_generator):\n","    for submission, path in submissions_generator:\n","        try:\n","            text = submission[\"title\"] + \" \" + submission[\"content\"]\n","        except:\n","            text = submission[\"body\"]\n","        yield (nlp.make_doc(text), submission, path)"]},{"source":["Instantiate the two generators:"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["submission_generator = get_submissions_generator(submission_files=submission_files)\n","submission_doc_generator = get_submission_doc_generator(submission_generator)"]},{"source":["Finally, one last generator that produces *matches* for each post - that is, a list of substances that were detected in each post."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["match_generator = (\n","    (matcher(text), submission, path, text)\n","    for text, submission, path in submission_doc_generator\n",")"]},{"source":["Finally, let's start the processing, saving files along the way with the added match information, and saving all data into one large dict:"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["submissions_dict = {}\n","for matches, submission, path, doc in match_generator:\n","    # Get the found mathches actual name\n","    matches_resolved = [matcher.vocab[match[0]].text for match in matches]\n","    # Eliminate duplicates\n","    matches_unique = list(set(matches_resolved))\n","    if not matches_unique:\n","        continue\n","    # Add to the submission and save back to file. Also add to large reddit dictionnary\n","    submission[\"matches\"] = matches_unique\n","    submission[\"n_of_words\"] = len(doc)\n","    with open(path, \"w\") as f:\n","        json.dump(submission, f)\n","    submissions_dict[submission[\"id\"]] = submission"]},{"source":["And save the large dict to file:"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save full reddit data to file:\n","with open(Config.Path.reddit_data_with_NER, \"w+\") as f:\n","    json.dump(submissions_dict, f)"]},{"source":["As a convenience for further analysis, let's save a  mapping between substances and posts in which they appear:"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["posts_per_substance = {substance: [] for substance in names}\n","for id in submissions_dict:\n","    for substance in submissions_dict[id][\"matches\"]:\n","        posts_per_substance[substance].append(id)\n","\n","with open(Config.Path.posts_per_substance, \"w+\") as f:\n","    json.dump(posts_per_substance, f)"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6-final"},"orig_nbformat":2,"kernelspec":{"name":"python3","display_name":"Python 3"}}}