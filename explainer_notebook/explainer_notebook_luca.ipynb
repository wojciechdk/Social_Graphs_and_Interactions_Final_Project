{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# get_ipython().run_line_magic(\"load_ext\", \"autoreload\")\n",
    "\n",
    "# get_ipython().run_line_magic(\"autoreload\", \"2\")\n",
    "# get_ipython().run_line_magic(\"matplotlib\", \"inline\")\n",
    "from tqdm import tqdm\n",
    "from config import Config\n",
    "import library_functions as lf\n",
    "import wojciech as w\n",
    "from ipywidgets import widgets\n",
    "import json\n",
    "import community\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Detecting Communities and visualizing them\n",
    "\n",
    " Let's load in our graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=108588.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "431aa794589e4abb90aebc7ab4d8acaa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4273.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c6719743634648218f0d17afb199f4f1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph_reddit = lf.create_graph_reddit(\n",
    "    max_drugs_in_post=6,  # Ignore posts that have too many substances in them, as they are likely noise\n",
    "    min_edge_occurrences_to_link=2,  # Include all mentions\n",
    "    include_link_contents=True,\n",
    "    include_node_contents=True,\n",
    "    min_content_length_in_characters=25,\n",
    ")\n",
    "graph_reddit = w.graph.largest_connected_component(graph_reddit)\n",
    "\n",
    "# Same for wikipedia, but here we also need to convert to an undirected graph\n",
    "graph_wiki = w.graph.largest_connected_component(lf.create_graph_wiki().to_undirected())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's look at our graphs. First, let's compute a layout, as it is an expensive operation and only needs to happe once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute positions, both taking edge weights into account and not doing so.\n",
    "positions_reddit_weighted = lf.get_fa2_layout(\n",
    "    graph=graph_reddit, edge_weight_attribute=\"count\"\n",
    ")\n",
    "positions_reddit_unweighted = lf.get_fa2_layout(graph=graph_reddit)\n",
    "\n",
    "positions_wiki = lf.get_fa2_layout(graph=graph_wiki)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " And let's just draw them as-is, with edge thickness corresponding to how often the two substances co-occur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# figure_weighted = lf.plotly_draw.draw_graph_plotly(\n",
    "#     graph=graph_reddit,\n",
    "#     positions=positions_reddit_weighted,\n",
    "#     node_size_attribute=\"degree\",  # Size nodes by degree\n",
    "#     edge_weight_attribute=\"count\",  # Size links by how often they appear, i.e. how often the two substances are mentionned together\n",
    "# )\n",
    "# figure_unweighted = lf.plotly_draw.draw_graph_plotly(\n",
    "#     graph=graph_reddit,\n",
    "#     positions=positions_reddit_unweighted,\n",
    "#     node_size_attribute=\"degree\",  # Size nodes by degree\n",
    "#     edge_weight_attribute=\"count\",  # Size links by how often they appear, i.e. how often the two substances are mentionned together\n",
    "# )\n",
    "# widgets.HBox([figure_unweighted, figure_weighted])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From these visualizations, it can be hard to see if there is any specific structure to the graph. Let's apply both the louvain and the infomap algorithm to detect communities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<networkx.classes.graph.Graph at 0x7fbcbe0e2970>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_graph, reddit_dendrogram = lf.assign_louvain_communities(graph_reddit)\n",
    "wiki_graph, wiki_dendrogram = lf.assign_louvain_communities(graph_wiki)\n",
    "lf.get_infomap_communities(reddit_graph, reddit_edge_weight=\"count\")\n",
    "lf.get_infomap_communities(wiki_graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# figure_weighted_colored_by_louvain_community = lf.plotly_draw.draw_graph_plotly(\n",
    "#     graph=graph_reddit,\n",
    "#     positions=positions_reddit_weighted,\n",
    "#     node_size_attribute=\"degree\",  # Size nodes by degree\n",
    "#     edge_weight_attribute=\"count\",  # Size links by how often they appear, i.e. how often the two substances are mentionned together\n",
    "#     node_color_attribute=\"louvain_community_reddit\",\n",
    "# )\n",
    "\n",
    "# figure_weighted_colored_by_infomap_community = lf.plotly_draw.draw_graph_plotly(\n",
    "#     graph=graph_reddit,\n",
    "#     positions=positions_reddit_weighted,\n",
    "#     node_size_attribute=\"degree\",  # Size nodes by degree\n",
    "#     edge_weight_attribute=\"count\",  # Size links by how often they appear, i.e. how often the two substances are mentionned together\n",
    "#     node_color_attribute=\"infomap_community\",\n",
    "# )\n",
    "\n",
    "# figure_unweighted_colored_by_community = lf.plotly_draw.draw_graph_plotly(\n",
    "#     graph=graph_reddit,\n",
    "#     positions=positions_reddit_unweighted,\n",
    "#     node_size_attribute=\"degree\", # Size nodes by degree\n",
    "#     edge_weight_attribute=\"count\", # Size links by how often they appear, i.e. how often the two substances are mentionned together\n",
    "#     node_color_attribute=\"louvain_community_reddit\"\n",
    "# )\n",
    "# widgets.HBox(\n",
    "#     [\n",
    "#         figure_weighted_colored_by_louvain_community,\n",
    "#         figure_weighted_colored_by_infomap_community,\n",
    "#     ]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Interesting! blablabla say some stuff about what the communities correspond to.\n",
    " Seems like the infomap algorithm isn't that good here, likely due to the fact that the network is highly connected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's look at the overlap between the communities found by louvain and the categories defined on wikipedia, to see if the algorithm picked up interesting information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_reddit = lf.inverse_communities_from_partition(\n",
    "    community.partition_at_level(reddit_dendrogram, 1)\n",
    ")\n",
    "communities_wiki = lf.inverse_communities_from_partition(\n",
    "    community.partition_at_level(wiki_dendrogram, 1)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Config.Path.all_categories_to_names_mapping, \"r\") as f:\n",
    "    categories_mapping = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlaps = {}\n",
    "for community in communities_reddit:\n",
    "    overlaps[community] = {}\n",
    "    for category in categories_mapping:\n",
    "        overlaps[community][category] = lf.overlap(\n",
    "            communities_reddit[community], categories_mapping[category]\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlaps_ranked = {}\n",
    "for overlap in overlaps:\n",
    "    overlaps_ranked[overlap] = sorted(\n",
    "        overlaps[overlap].items(),\n",
    "        key=lambda x: x[1][\"overlap_proportion\"],\n",
    "        reverse=True,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for overlap in overlaps_ranked:\n",
    "    print(\n",
    "        f\"Community {overlap}: \\n\\nNumber of elements in community: {len(communities_reddit[overlap])}.\"\n",
    "    )\n",
    "    print(f\"Example members: {', '.join(communities_reddit[overlap][0:10])}.\\n\")\n",
    "    print(\"Categories with largest overlap:\")\n",
    "    for category, overlap_data in overlaps_ranked[overlap][0:5]:\n",
    "        print(\n",
    "            f\"Category '{category}' ({len(categories_mapping[category])} members):         \\n\\t{overlap_data['overlap_proportion']*100:.1f}% overlap,        \\n\\t{overlap_data['proportion_in_category']*100:.1f}% of this community contained in this category,        \\n\\t{overlap_data['proportion_in_community']*100:.2f}% of the category contained in this community\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " .... diccussion....\n",
    "\n",
    "\n",
    " The categories above are the raw categories extracted from wikipedia: there is 1800+ of them in our dataset, and they are very granular. Let's also try to do the same operation, but resolving categories to two \"root categories\":\n",
    "\n",
    " - https://en.wikipedia.org/wiki/Category:Psychoactive_drugs_by_mechanism_of_action\n",
    " - https://en.wikipedia.org/wiki/Category:Drugs_by_psychological_effects\n",
    "\n",
    " To do so, we semi-manually mapped all categories that are sub categories of one of those two categories to the corresponding top-level category: this way, for example, \"Masticatories\" is mapped to \"Herbal and Fungal Stimulants\" which is mapped to \"Stimulants\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Config.Path.wiki_mechanism_categories, \"r\") as f:\n",
    "    mechanism_categories = json.load(f)\n",
    "\n",
    "wiki_data = lf.load_data_wiki()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.assign_root_categories(\n",
    "    reddit_graph,\n",
    "    wiki_data=wiki_data,\n",
    "    mapping=mechanism_categories,\n",
    "    name=\"wiki_category_mechanisms\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlaps = {}\n",
    "for community in communities_reddit:\n",
    "    overlaps[community] = {}\n",
    "    for category in mechanism_categories:\n",
    "        overlaps[community][category] = lf.overlap(\n",
    "            communities_reddit[community], categories_mapping[category]\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=347.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "649d42244ae4499098dd173de0bba71d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-33-c15715260431>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mlf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0massign_lemmas\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgraph_reddit\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mlf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0massign_lemmas\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgraph_wiki\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/MEGA/DTU/Q2/social_graphs/final_project_new/library_functions/text_analysis.py\u001B[0m in \u001B[0;36massign_lemmas\u001B[0;34m(graph)\u001B[0m\n\u001B[1;32m     22\u001B[0m     ), \"The graph does not contain node contents.\"\n\u001B[1;32m     23\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mnode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgraph\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnodes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m         \u001B[0mdoc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnlp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmake_doc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\" \"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"contents\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     25\u001B[0m         \u001B[0mlemmas\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlemma_\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlower\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdoc\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_punct\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0mgraph\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnodes\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mnode\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"lemmas\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlemmas\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/MEGA/DTU/Q2/social_graphs/final_project_new/venv/lib/python3.8/site-packages/spacy/language.py\u001B[0m in \u001B[0;36mmake_doc\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m    465\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    466\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mmake_doc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 467\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    468\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    469\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_format_docs_and_golds\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdocs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgolds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32mtokenizer.pyx\u001B[0m in \u001B[0;36mspacy.tokenizer.Tokenizer.__call__\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mtokenizer.pyx\u001B[0m in \u001B[0;36mspacy.tokenizer.Tokenizer._tokenize\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mtokenizer.pyx\u001B[0m in \u001B[0;36mspacy.tokenizer.Tokenizer._attach_tokens\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mvocab.pyx\u001B[0m in \u001B[0;36mspacy.vocab.Vocab.get\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mvocab.pyx\u001B[0m in \u001B[0;36mspacy.vocab.Vocab._new_lexeme\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m~/MEGA/DTU/Q2/social_graphs/final_project_new/venv/lib/python3.8/site-packages/spacy/lang/lex_attrs.py\u001B[0m in \u001B[0;36mlower\u001B[0;34m(string)\u001B[0m\n\u001B[1;32m    175\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    176\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 177\u001B[0;31m \u001B[0;32mdef\u001B[0m \u001B[0mlower\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstring\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    178\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mstring\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlower\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    179\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "lf.assign_lemmas(graph_reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The graph does not contain node contents.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-34-daffc264e923>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mlf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0massign_lemmas\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgraph_wiki\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/MEGA/DTU/Q2/social_graphs/final_project_new/library_functions/text_analysis.py\u001B[0m in \u001B[0;36massign_lemmas\u001B[0;34m(graph)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;31m#%%\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0massign_lemmas\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgraph\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mnx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mGraph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msave_spacy_docs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 20\u001B[0;31m     assert (\n\u001B[0m\u001B[1;32m     21\u001B[0m         \u001B[0;34m\"contents\"\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgraph\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnodes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m     ), \"The graph does not contain node contents.\"\n",
      "\u001B[0;31mAssertionError\u001B[0m: The graph does not contain node contents."
     ]
    }
   ],
   "source": [
    "lf.assign_lemmas(graph_wiki)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "graph_wiki.nodes(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lf.assign_tfs(graph_reddit)\n",
    "lf.assign_tfs(graph_wiki)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.assign_idfs(graph_reddit)\n",
    "lf.assign_idfs(graph_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.assign_tf_idfs(graph_reddit)\n",
    "lf.assign_tf_idfs(graph_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# nx.readwrite.gpickle.write_gpickle(graph_reddit, Config.Path.reddit_with_text)\n",
    "graph_reddit = nx.readwrite.gpickle.read_gpickle(Config.Path.reddit_with_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for node in tqdm(graph_reddit.nodes):\n",
    "    wc = lf.wordcloud_from_node(graph_reddit, node)\n",
    "    wc.to_file(Config.Path.shared_data_folder / \"wordclouds\" / \"reddit\" / f\"{node.replace('/','_')}.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 347/347 [12:01<00:00,  2.08s/it]\n"
     ]
    }
   ],
   "source": [
    "for node in tqdm(graph_wiki.nodes):\n",
    "    wc = lf.wordcloud_from_node(graph_wiki, node)\n",
    "    wc.to_file(Config.Path.shared_data_folder / \"wordclouds\" / \"wiki\" / f\"{node.replace('/','_')}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[('i', 0.03825586966938189),\n ('be', 0.03525994250119789),\n ('a', 0.028597268806899855),\n ('and', 0.026451844753234307),\n ('the', 0.02609008145663632)]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pycharm-2954a4aa",
   "language": "python",
   "display_name": "PyCharm (final_project_new)"
  }
 }
}